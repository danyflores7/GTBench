# ðŸ“Š Script de ComparaciÃ³n de Configuraciones

## ðŸŽ¯ DescripciÃ³n

`compare_configurations.py` es un script que genera un **reporte HTML interactivo** comparando el rendimiento de GPT-OSS-20B en diferentes configuraciones de razonamiento y estrategias de prompt.

## âœ¨ CaracterÃ­sticas

El reporte incluye:

### ðŸ“ˆ **Visualizaciones Interactivas:**
- GrÃ¡fico de barras: Tasa de victoria por configuraciÃ³n
- GrÃ¡fico de lÃ­neas: Tiempo promedio de ejecuciÃ³n
- GrÃ¡fico de lÃ­neas: Tokens promedio consumidos

### ðŸ“Š **Tabla Comparativa Detallada:**
- ConfiguraciÃ³n (Razonamiento + Estrategia de Prompt)
- Resultados por juego
- NÃºmero de victorias, derrotas y empates
- Porcentaje de victoria
- Partidas normales vs anormales
- Tiempo promedio por partida
- Tokens promedio consumidos

### ðŸŽ¨ **DiseÃ±o Moderno:**
- Interfaz responsiva y moderna
- GrÃ¡ficos interactivos con Chart.js
- CÃ³digos de colores para fÃ¡cil interpretaciÃ³n
- Badges para identificar configuraciones

## ðŸš€ Uso

### **BÃ¡sico:**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-no-reasoning-simple/ \
  experiments/test-no-reasoning-cot/ \
  experiments/test-reasoning-low-simple/
```

### **Con archivo de salida personalizado:**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-*/ \
  --output mi_reporte.html
```

### **Todas las carpetas de experimentos:**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-no-reasoning-simple/ \
  experiments/test-no-reasoning-cot/ \
  experiments/test-no-reasoning-tot/ \
  experiments/test-reasoning-low-simple/ \
  experiments/test-reasoning-low-cot/ \
  experiments/test-reasoning-low-tot/ \
  experiments/test-reasoning-medium-simple/ \
  experiments/test-reasoning-medium-cot/ \
  experiments/test-reasoning-medium-tot/ \
  --output comparison_full.html
```

### **Usando wildcards (bash):**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-*/ \
  --output comparison_all.html
```

## ðŸ“‹ Opciones

```
positional arguments:
  folders              Carpetas de experimentos a comparar

options:
  -h, --help           Muestra este mensaje de ayuda
  --output, -o FILE    Archivo HTML de salida (default: comparison_report.html)
```

## ðŸ“‚ Estructura de Entrada

El script busca automÃ¡ticamente archivos `.jsonl` en las carpetas especificadas y extrae:

- **ConfiguraciÃ³n de razonamiento:** Desde el nombre de la carpeta
  - `test-no-reasoning-*` â†’ Sin Razonamiento
  - `test-reasoning-low-*` â†’ Razonamiento Bajo
  - `test-reasoning-medium-*` â†’ Razonamiento Medio

- **Estrategia de prompt:** Desde el nombre de la carpeta
  - `*-simple` â†’ Prompt Simple
  - `*-cot` â†’ Chain of Thought (CoT)
  - `*-tot` â†’ Tree of Thoughts (ToT)

- **Juegos:** Desde la estructura de subdirectorios
  - `experiments/test-*/tictactoe/*.jsonl`
  - `experiments/test-*/kuhn_poker/*.jsonl`
  - etc.

## ðŸ“Š MÃ©tricas Calculadas

Para cada configuraciÃ³n y juego:

| MÃ©trica | DescripciÃ³n |
|---------|-------------|
| **Partidas** | Total de partidas jugadas |
| **Victorias** | NÃºmero de partidas ganadas por GPT-OSS |
| **Derrotas** | NÃºmero de partidas perdidas |
| **Empates** | NÃºmero de partidas empatadas |
| **% Victoria** | Porcentaje de victorias (victorias/total) |
| **Normal/Anormal** | Partidas completadas correctamente vs con errores |
| **Tiempo Promedio** | DuraciÃ³n promedio por partida en segundos |
| **Tokens Promedio** | NÃºmero promedio de tokens consumidos |

## ðŸŽ¨ InterpretaciÃ³n de Colores

### **Tasa de Victoria:**
- ðŸŸ¢ **Verde** (â‰¥60%): Excelente rendimiento
- ðŸŸ¡ **Amarillo** (40-59%): Rendimiento moderado
- ðŸ”´ **Rojo** (<40%): Bajo rendimiento

### **Badges de ConfiguraciÃ³n:**
- ðŸ”µ **Azul**: Sin razonamiento
- ðŸŸ¡ **Amarillo**: Razonamiento bajo
- ðŸ”´ **Rojo**: Razonamiento medio

- ðŸ’™ **Azul claro**: Prompt Simple
- ðŸ’š **Verde**: Chain of Thought (CoT)
- ðŸ’— **Rosa**: Tree of Thoughts (ToT)

## ðŸ’¡ Ejemplos

### **Comparar solo configuraciones sin razonamiento:**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-no-reasoning-*/ \
  -o no_reasoning_comparison.html
```

### **Comparar solo estrategia CoT:**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-*-cot/ \
  -o cot_comparison.html
```

### **Comparar razonamiento bajo:**

```bash
./.venv/bin/python scripts/compare_configurations.py \
  experiments/test-reasoning-low-*/ \
  -o low_reasoning_comparison.html
```

## ðŸ”§ SoluciÃ³n de Problemas

### **Error: No se encontraron archivos JSONL**
- Verifica que las carpetas contengan archivos `.jsonl`
- AsegÃºrate de ejecutar los experimentos primero

### **GrÃ¡ficos no se muestran**
- Verifica tu conexiÃ³n a internet (Chart.js se carga desde CDN)
- O descarga Chart.js localmente

### **Configuraciones no detectadas correctamente**
- Verifica que los nombres de carpetas sigan el formato:
  - `test-[no-reasoning|reasoning-low|reasoning-medium]-[simple|cot|tot]`

## ðŸ“ Notas

- El script funciona con cualquier juego de OpenSpiel compatible
- Los resultados se calculan automÃ¡ticamente desde los archivos JSONL
- El HTML generado es completamente autÃ³nomo (excepto Chart.js)
- Puedes compartir el archivo HTML directamente

## ðŸŽ¯ PrÃ³ximos Pasos

DespuÃ©s de analizar el reporte:

1. **Identificar la mejor configuraciÃ³n** por juego
2. **Comparar eficiencia** (tokens vs rendimiento)
3. **Analizar trade-offs** (tiempo vs precisiÃ³n)
4. **Optimizar configuraciones** basÃ¡ndose en resultados

---

**Generado por GTBench Analysis Tool**
